#!/usr/bin/env python3
"""
Expert Directory Builder

Fetches per-expert stats from the Airtable "Sales Wisdom" table,
merges with influencers.json metadata, updates docs/experts.md,
and pushes a new "Expert Directory" table to Airtable.

Usage:
    ./run.sh build_expert_directory
"""

import json
import logging
import re
import time
from collections import Counter, defaultdict
from pathlib import Path

from pyairtable import Api

from config import (
    AIRTABLE_API_KEY,
    AIRTABLE_BASE_ID,
    AIRTABLE_TABLE_NAME,
    DEAL_STAGES,
    PROJECT_ROOT,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Paths
INFLUENCERS_FILE = PROJECT_ROOT / "data" / "influencers.json"
EXPERTS_MD = PROJECT_ROOT / "docs" / "experts.md"
MEMORY_EXPERTS_MD = Path.home() / ".claude" / "projects" / "-Users-amitgoldstein-projects-salescoach" / "memory" / "experts.md"

# Airtable table names
DIRECTORY_TABLE_NAME = "Expert Directory"
COMPANY_TABLE_NAME = "Company Directory"


def fetch_airtable_stats() -> dict[str, dict]:
    """Fetch all records from Sales Wisdom and compute per-expert stats.

    Returns dict keyed by expert name with:
        li_posts: int, stages: list[str], insights: int
    """
    if not AIRTABLE_API_KEY or not AIRTABLE_BASE_ID:
        logger.error("AIRTABLE_API_KEY or AIRTABLE_BASE_ID not set")
        return {}

    base_id = AIRTABLE_BASE_ID.split("/")[0]
    api = Api(AIRTABLE_API_KEY)
    table = api.table(base_id, AIRTABLE_TABLE_NAME)

    logger.info("Fetching all records from Sales Wisdom...")
    records = table.all()
    logger.info(f"Fetched {len(records)} records")

    # Aggregate per expert
    stats = defaultdict(lambda: {"li_posts": 0, "stages": set(), "insights": 0})

    for record in records:
        fields = record.get("fields", {})
        name = fields.get("Influencer", "").strip()
        if not name:
            continue

        stats[name]["insights"] += 1

        source_type = fields.get("Source Type", "")
        if source_type == "LinkedIn":
            stats[name]["li_posts"] += 1

        primary_stage = fields.get("Primary Stage", "").strip()
        if primary_stage:
            stats[name]["stages"].add(primary_stage)

    # Sort stages by DEAL_STAGES order
    stage_order = {stage: i for i, stage in enumerate(DEAL_STAGES)}
    for name in stats:
        sorted_stages = sorted(
            stats[name]["stages"],
            key=lambda s: stage_order.get(s, 999),
        )
        stats[name]["stages"] = sorted_stages

    return dict(stats)


def count_target_videos() -> dict[str, int]:
    """Count TARGET_VIDEOS entries per expert from collect_youtube.py."""
    collect_path = PROJECT_ROOT / "tools" / "collect_youtube.py"
    with open(collect_path) as f:
        content = f.read()
    # Match tuples: ("VIDEO_ID", "Expert Name", "Channel")
    tuples = re.findall(r'\(\s*"[A-Za-z0-9_-]{11}"\s*,\s*"([^"]+)"\s*,', content)
    return dict(Counter(tuples))


def load_experts() -> list[dict]:
    """Load all experts from influencers.json with metadata."""
    video_counts = count_target_videos()

    with open(INFLUENCERS_FILE) as f:
        data = json.load(f)

    experts = []
    for inf in data["influencers"]:
        status = inf.get("status", "")
        if status not in ("active", "company"):
            continue

        # Extract LinkedIn handle from platforms dict
        platforms = inf.get("platforms", {})
        li_handle = platforms.get("linkedin", {}).get("handle", "")

        # Extract YouTube channel name
        yt_data = platforms.get("youtube", {})
        yt_channel = yt_data.get("handle", "") or yt_data.get("channel_id", "")

        # Video count from TARGET_VIDEOS in collect_youtube.py (curated videos)
        video_count = video_counts.get(inf["name"], 0)

        # Focus from metadata.focus_areas (list)
        focus_areas = inf.get("metadata", {}).get("focus_areas", [])
        focus = ", ".join(focus_areas) if focus_areas else ""

        experts.append({
            "name": inf["name"],
            "status": status,
            "linkedin": li_handle,
            "focus": focus,
            "yt_channel": yt_channel,
            "video_count": video_count,
        })

    return experts


def build_experts_md(experts: list[dict], stats: dict) -> str:
    """Generate the experts.md markdown content with enriched columns."""
    lines = []
    lines.append("# Expert Directory (51 total: 48 active + 3 company)")
    lines.append("")
    lines.append("Last updated: auto-generated by `build_expert_directory.py`")
    lines.append("")
    lines.append("## All Experts")
    lines.append("")
    lines.append("| # | Name | LinkedIn | Focus | YT Channel | Videos | LI Posts | Stages | Insights |")
    lines.append("|---|------|----------|-------|------------|--------|----------|--------|----------|")

    # Group experts by source
    original_16 = [
        "Ian Koniak", "Morgan J Ingram", "Samantha McKenna", "John Barrows",
        "Josh Braun", "Chris Voss", "Jeb Blount", "Daniel Disney",
        "Devin Reed", "Armand Farrokh", "Nick Cegelski", "Kyle Coleman",
        "Will Aitken", "Florin Tatulea", "Nate Nasralla", "Gal Aga",
    ]
    monday_crm = [
        "Anthony Iannarino", "Giulio Segantini", "Mark Hunter", "Jill Konrath",
        "Shari Levitin", "Jim Keenan", "Tiffani Bova", "Amy Volas",
        "Ron Kimhi", "Chris Orlob", "Becc Holland", "Jen Allen-Knuth",
        "Alexandra Carter", "Kwame Christian", "Mo Bunnell",
        "Rosalyn Santa Elena", "Mark Kosoglow", "Scott Leese",
    ]
    proposify = [
        "Sarah Brazier", "Jesse Gittler", "Chantel George", "Bryan Tucker",
        "Colin Specter", "Kevin Dorsey", "Belal Batrawy", "Caroline Celis",
        "Julie Hansen", "Hannah Ajikawo", "Justin Michael", "Erica Franklin",
        "Maria Bross", "Niraj Kapur",
    ]
    companies = ["30MPC", "Gong.io", "Pavilion"]

    expert_lookup = {e["name"]: e for e in experts}
    num = 0

    def add_section(title):
        lines.append(f"| | **{title}** | | | | | | | |")

    def add_expert(name):
        nonlocal num
        e = expert_lookup.get(name)
        if not e:
            return
        num += 1
        s = stats.get(name, {"li_posts": 0, "stages": [], "insights": 0})
        li = e["linkedin"]
        li_link = f"[{li}](https://linkedin.com/in/{li})" if li and "/" not in li else f"[{li}](https://linkedin.com/company/{li})" if li else ""
        yt = e["yt_channel"] if e["yt_channel"] else "—"
        stages_text = ", ".join(s["stages"]) if s["stages"] else "—"
        lines.append(
            f"| {num} | {name} | {li_link} | {e['focus']} | {yt} | {e['video_count']} "
            f"| {s['li_posts']} | {stages_text} | {s['insights']} |"
        )

    add_section("ORIGINAL 16")
    for name in original_16:
        add_expert(name)

    add_section("MONDAY CRM TOP 25 (18 new, added 2026-02-06)")
    for name in monday_crm:
        add_expert(name)

    add_section("PROPOSIFY BEST SALES VOICES (14 new, added 2026-02-06)")
    for name in proposify:
        add_expert(name)

    add_section("COMPANY PROFILES")
    for name in companies:
        e = expert_lookup.get(name)
        if not e:
            continue
        num += 1
        s = stats.get(name, {"li_posts": 0, "stages": [], "insights": 0})
        li = e["linkedin"]
        # Companies use /company/ URLs
        if name in ("30MPC", "Pavilion"):
            li_link = f"[{li}](https://linkedin.com/company/{li})"
        else:
            li_link = f"[{li}](https://linkedin.com/in/{li})"
        yt = e["yt_channel"] if e["yt_channel"] else "—"
        stages_text = ", ".join(s["stages"]) if s["stages"] else "—"
        lines.append(
            f"| {num} | {name} | {li_link} | {e['focus']} | {yt} | {e['video_count']} "
            f"| {s['li_posts']} | {stages_text} | {s['insights']} |"
        )

    # Summary stats
    total_li = sum(s.get("li_posts", 0) for s in stats.values())
    total_insights = sum(s.get("insights", 0) for s in stats.values())
    total_videos = sum(e["video_count"] for e in experts)

    lines.append("")
    lines.append("## Summary Stats")
    lines.append(f"- **Total insights in Airtable**: {total_insights}")
    lines.append(f"- **Total LinkedIn posts**: {total_li}")
    lines.append(f"- **Total YouTube videos**: {total_videos}")
    lines.append(f"- **Experts with Airtable data**: {sum(1 for s in stats.values() if s.get('insights', 0) > 0)}")
    lines.append("")
    lines.append("## Sources")
    lines.append("- Original 16: manually curated (pre-2026-02-06)")
    lines.append("- Monday CRM Top 25: https://monday.com/blog/sales-crm/top-25-sales-influencers/")
    lines.append("- Proposify Best Sales Voices: https://www.proposify.com/blog/best-sales-voices")
    lines.append("")

    return "\n".join(lines)


def _push_to_table(base, api, base_id, table_name: str, fields: list[dict],
                    records_data: list[dict]) -> None:
    """Create a table (if needed) and upsert records into it."""
    # Try to create the table
    try:
        logger.info(f"Creating table '{table_name}'...")
        table = base.create_table(table_name, fields)
        logger.info(f"Created table: {table_name}")
    except Exception as e:
        if "DUPLICATE_TABLE_NAME" in str(e) or "already exists" in str(e).lower():
            logger.info(f"Table '{table_name}' already exists, will upsert records")
            table = api.table(base_id, table_name)
        else:
            raise

    # Check for existing records (for upsert by Name)
    existing = table.all()
    existing_by_name = {
        r["fields"].get("Name"): r["id"]
        for r in existing
        if r["fields"].get("Name")
    }
    logger.info(f"Found {len(existing_by_name)} existing records in {table_name}")

    to_create = []
    to_update = []
    for rec in records_data:
        name = rec.get("Name", "")
        if name in existing_by_name:
            to_update.append({"id": existing_by_name[name], "fields": rec})
        else:
            to_create.append({"fields": rec})

    BATCH_SIZE = 10
    created = updated = 0

    for i in range(0, len(to_create), BATCH_SIZE):
        batch = to_create[i:i + BATCH_SIZE]
        try:
            table.batch_create([r["fields"] for r in batch])
            created += len(batch)
        except Exception as e:
            logger.error(f"Error creating batch in {table_name}: {e}")
        time.sleep(0.5)

    for i in range(0, len(to_update), BATCH_SIZE):
        batch = to_update[i:i + BATCH_SIZE]
        try:
            table.batch_update(batch)
            updated += len(batch)
        except Exception as e:
            logger.error(f"Error updating batch in {table_name}: {e}")
        time.sleep(0.5)

    print(f"  {table_name}: {created} created, {updated} updated")


def push_expert_directory(experts: list[dict], stats: dict) -> None:
    """Push experts and companies to separate Airtable tables."""
    if not AIRTABLE_API_KEY or not AIRTABLE_BASE_ID:
        logger.error("AIRTABLE_API_KEY or AIRTABLE_BASE_ID not set")
        return

    base_id = AIRTABLE_BASE_ID.split("/")[0]
    api = Api(AIRTABLE_API_KEY)
    base = api.base(base_id)

    # Shared field definitions (no Status column — separation handles it)
    fields = [
        {"name": "Name", "type": "singleLineText"},
        {"name": "LinkedIn Handle", "type": "singleLineText"},
        {"name": "Focus", "type": "singleLineText"},
        {"name": "YT Channel", "type": "singleLineText"},
        {"name": "Video Count", "type": "number", "options": {"precision": 0}},
        {"name": "LI Post Count", "type": "number", "options": {"precision": 0}},
        {"name": "Sales Stages", "type": "multilineText"},
        {"name": "Insight Count", "type": "number", "options": {"precision": 0}},
    ]

    # Split experts from companies
    expert_records = []
    company_records = []

    for expert in experts:
        name = expert["name"]
        s = stats.get(name, {"li_posts": 0, "stages": [], "insights": 0})
        stages_text = ", ".join(s["stages"]) if s["stages"] else ""

        record_data = {
            "Name": name,
            "LinkedIn Handle": expert["linkedin"],
            "Focus": expert["focus"],
            "YT Channel": expert["yt_channel"],
            "Video Count": expert["video_count"],
            "LI Post Count": s["li_posts"],
            "Sales Stages": stages_text,
            "Insight Count": s["insights"],
        }

        if expert["status"] == "company":
            company_records.append(record_data)
        else:
            expert_records.append(record_data)

    print()
    _push_to_table(base, api, base_id, DIRECTORY_TABLE_NAME, fields, expert_records)
    _push_to_table(base, api, base_id, COMPANY_TABLE_NAME, fields, company_records)


def main():
    # 1. Fetch stats from Airtable
    stats = fetch_airtable_stats()
    if not stats:
        logger.warning("No Airtable stats found. Experts.md will show zeros for LI/insight columns.")

    # 2. Load expert metadata
    experts = load_experts()
    logger.info(f"Loaded {len(experts)} experts from influencers.json")

    # 3. Build and write experts.md
    md_content = build_experts_md(experts, stats)

    EXPERTS_MD.parent.mkdir(parents=True, exist_ok=True)
    with open(EXPERTS_MD, "w") as f:
        f.write(md_content)
    logger.info(f"Wrote {EXPERTS_MD}")

    if MEMORY_EXPERTS_MD.parent.exists():
        with open(MEMORY_EXPERTS_MD, "w") as f:
            f.write(md_content)
        logger.info(f"Wrote {MEMORY_EXPERTS_MD}")

    # 4. Push to Airtable
    try:
        push_expert_directory(experts, stats)
    except Exception as e:
        logger.error(f"Airtable push failed: {e}")
        print(f"\nAirtable push FAILED: {e}")
        print("You may need to update the API token scope (add schema.bases:write).")

    # 5. Print summary
    print("\n" + "=" * 60)
    print("EXPERT DIRECTORY SUMMARY")
    print("=" * 60)
    total_insights = sum(s.get("insights", 0) for s in stats.values())
    total_li = sum(s.get("li_posts", 0) for s in stats.values())
    experts_with_data = sum(1 for e in experts if stats.get(e["name"], {}).get("insights", 0) > 0)
    print(f"Experts in directory:   {len(experts)}")
    print(f"Experts with insights:  {experts_with_data}")
    print(f"Total insights:         {total_insights}")
    print(f"Total LI posts:         {total_li}")
    print()

    # Per-expert breakdown
    for e in experts:
        s = stats.get(e["name"], {"li_posts": 0, "stages": [], "insights": 0})
        stages = len(s["stages"])
        print(f"  {e['name']:30s}  LI:{s['li_posts']:3d}  Stages:{stages:2d}  Insights:{s['insights']:3d}")


if __name__ == "__main__":
    main()
